% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Machine Learning block 1 Lab2 Group A13},
  pdfauthor={Group A13: Arash Haratian, Connor Turner, Yi Hung Chen},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Machine Learning block 1 Lab2 Group A13}
\author{Group A13: Arash Haratian, Connor Turner, Yi Hung Chen}
\date{2022-11-30}

\begin{document}
\maketitle

\textbf{Statement of Contribution:} \emph{Assignment 1 was contributed
by Yi Hung, Assignment 2 was contributed by Arash, and Assignment 3 was
contributed by Connor. Each assignment was then discussed in detail with
the group before piecing together each secton of the final report.}

\hypertarget{assignment-1.-explicit-regularization}{%
\section{Assignment 1. Explicit
regularization}\label{assignment-1.-explicit-regularization}}

For this assignment, we are given data ``tecator.csv'' that contains the
results of study aimed to investigate whether a near infrared absorbance
spectrum can be used to predict the fat content of samples of meat.

\hypertarget{q1.}{%
\paragraph{Q1.}\label{q1.}}

We begin by divide data randomly to train and test set(50/50). We then
use the training data to train a linear regression model with all the
absorbance characteristics (Channels) as features. Below is the
underlying probabilistic model of linear regression

\[y=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_{100}x_{100}+\epsilon\]
\[y|x\sim N(\theta^Tx,\sigma^2)\]

where \[y=Fat\] \[ \theta={\theta_0,...,\theta_{100}}\]
\[ x=\{ 1,Channel_1,Channel_2,...,Channel_{100} \}\]
\[ \epsilon \ is\ the\ error\ term\]

We estimate mean squared errors(MSE) for both training data and testing
data. We obtain 0.0057091 for training data and 722.4294193 for testing
data. The large MSE on testing data indicates the linear regression
model is overfitting. To improve this, we are going to use Lasso and
Ridge regression in this assignment.

\hypertarget{q2.}{%
\paragraph{Q2.}\label{q2.}}

We then switch to use LASSO regression model. Which the cost function
that should be optimized is.
\[\hat{\theta^{lasso}} = \{\frac{1}{n}\sum_{i = 1}^{n}(y_i-\theta_0-\sum_{j = 1}^{p}\theta_jx_{ij})^2 + \lambda\sum_{j = 1}^{p}|\theta_j| \}\]

\hypertarget{q3.}{%
\paragraph{Q3.}\label{q3.}}

We fit the LASSO regression model to the training data. The dependency
of the regression coefficients on the log(\(\lambda\)) is shown below in
Figure 1. As the plot shown, the LASSO will drop some coefficients as
\(\lambda\) increase (set the values to 0). Interestingly, when
\(\lambda\) change, some coefficient will reappear.

\begin{center}\includegraphics[width=0.5\linewidth]{Lab2-Group_files/figure-latex/unnamed-chunk-2-1} \end{center}

If we want to select a model with only three features, we can choose
\(\lambda\) around 0.9512294 to 0.7046881.(Note: The range is choosen by
dierectly observed with the plot(vertical lines). Additionally, by using
default lambdas in glmnet, we calculated \(\lambda\) = 0.8530452,
0.777263, 0.7082131 will gives coefficient for only three features.
However, as the plot shown, the lambdas should be in a range not
district values.)

\hypertarget{q4.}{%
\paragraph{Q4.}\label{q4.}}

We then repeat the privious step, but instead of using Lasso Regression,
we use Ridge Regression to compare both method.

As the Figure 2 shown. Compare to Lasso Regression, the penalty term of
Ridge Regression will not ``collapse'' to zero. Also, the coefficient
value of Ridge Regression scale slower when lambda increase compared to
Lasso Regression.

\begin{center}\includegraphics[width=0.5\linewidth]{Lab2-Group_files/figure-latex/unnamed-chunk-3-1} \end{center}

\hypertarget{q5.}{%
\paragraph{Q5.}\label{q5.}}

We use cv.glmnet function to do cross-validation for Lasso model. We do
not specify how many folds being use, instead we use the default number
of folds to compute. As the Figure 3 shown, the MSE start increasing
dramatically as log(\(\lambda\)) approach -2, which means the the model
become less usable, this might result from too many coefficient being
ditched by Lasso regression as lambda increase.

\begin{center}\includegraphics[width=0.5\linewidth]{Lab2-Group_files/figure-latex/unnamed-chunk-4-1} \end{center}

We obtain the best \(\lambda\) =0.0574453 with 7 variables
chosen(Channel13, Channel14, Channel15, Channel16, Channel40, Channel41,
Channel51). According to Figure 3, the optimal \(\lambda\)
(log(\(\lambda\)) =-2.8569213) is not statistically significantly better
than log\(\lambda\) = -4, as they have similar MSE.

Finally, we plot the scatter plot (Figure 4) with true ``Fat'' value
from test data on the X-axis and predict value on the Y-axis. We can
observed that the prediction is much better compare to linear
regression(MSE = 13.2997953). Note: We also include a line x=y which
indicate where the perfect prediction should lay on.

\begin{center}\includegraphics{Lab2-Group_files/figure-latex/unnamed-chunk-6-1} \end{center}

\hypertarget{assignment-2}{%
\subsection{Assignment 2}\label{assignment-2}}

In this assignment we are given the data related with direct marketing
campaigns of a Portuguese banking institution, and we have to use
decision tree model to classify the target variable \texttt{y} using 15
features. The target variable has two levels: \texttt{yes} and
\texttt{no}.

We begin by splitting data 40/30/30 into training, validation, and
testing sets. We then use the training data to train 3 different
decision trees with 3 different settings:

\textbf{1. First Decision Tree:}

The first model is a decision tree with default settings of the
\texttt{tree::tree()}. The default values are as follows: -
\texttt{mincut\ =\ 5} - \texttt{minsize\ =\ 10} -
\texttt{mindev\ =\ 0.01}

The training and test errors for this model are 0.1048 and 0.1093
respectively.

\textbf{2. Second Decision Tree:} The second model is a decision tree
with a constraint of the size of each node; In fact, the smallest
allowed node size should be equal to 7000. To set this setting, we
should pass \texttt{tree::tree.control(minsize\ =\ 7000)} to the
\texttt{control} argument of the \texttt{tree::tree()}.

The training and test errors for the second model are 0.1048 and 0.1093
respectively.

\textbf{3. Third Decision Tree:}

The third model is a decision we want to set the minimum deviance to
0.0005, and it can be done in the same way as the second model, by
passing \texttt{tree::tree.control(mindev\ =\ 0.0005)} to the
\texttt{control} argument of the \texttt{tree::tree()}.

The training and test errors for the third model are 0.094 and 0.1119
respectively.

\begin{longtable}[]{@{}llll@{}}
\toprule()
different values & First Model & Second Model & Third Model \\
\midrule()
\endhead
Size of Trees & 6 & 5 & 122 \\
Number of feature used & 4 & 3 & 13 \\
Training Error & 0.1048 & 0.1048 & 0.094 \\
Testing Error & 0.1093 & 0.1093 & 0.1119 \\
\bottomrule()
\end{longtable}

Based on the information in the table above, first model and second
model have the same error rate. However, both the number of terminal
nodes and number of feature used to split for the second model is 1 less
that the first model. In our opinion the second model is more proper for
interpreting the structure of the tree, but for predicting new values,
the first model may preform better as it is more complex compare to the
second model. The third model is overfitted on the training data and it
is too complex; as the result, it may preform poorly for predicting new
values.

Now we can prune the more complex model, in this case the third
model(minimum deviance to 0.0005), to reach to a more optimal model. By
doing so, we may end up with a model which captures the general
structure better than other models while it is not overfitted on the
training dataset. In other words, by pruning a fully-grown tree, we are
mitigating the overfitting (or high variance) problem. The following
figure shows the dependence of deviances for the training and the
validation data in the number of leaves:

\includegraphics{Lab2-Group_files/figure-latex/unnamed-chunk-12-1.pdf}

As it is evidence in the plot, the deviance of the training data set is
decreasing by increasing the number of leaves(complexity), whereas the
deviance of the validation data set is decreasing at first but then it
starts to increase. This denotes that the model will overfit the by
increasing the number of terminal nodes. It worth to note that the
deviance of the training data set is higher than validation dataset
because deviance is proportional to the number of observations in the
dataset.

The best number of the leaves is 22.

The important variables in the optimal tree are as follows: -
\texttt{poutcome} - \texttt{month} - \texttt{contact} - \texttt{pdays} -
\texttt{age} - \texttt{day} - \texttt{balance} - \texttt{housing} -
\texttt{job} Also we can see the the structure of the tree:

\begin{verbatim}
## node), split, n, deviance, yval, (yprob)
##       * denotes terminal node
## 
##    1) root 18084 12850.00 no ( 0.88576 0.11424 )  
##      2) poutcome: failure,other,unknown 17468 11030.00 no ( 0.90422 0.09578 )  
##        4) month: apr,aug,feb,jan,jul,jun,may,nov 16828  9772.00 no ( 0.91520 0.08480 )  
##          8) contact: unknown 5130  1599.00 no ( 0.96374 0.03626 )  
##           16) month: jul,jun,may 5074  1502.00 no ( 0.96610 0.03390 ) *
##           17) month: apr,aug,feb,jan,nov 56    62.98 no ( 0.75000 0.25000 ) *
##          9) contact: cellular,telephone 11698  7914.00 no ( 0.89391 0.10609 )  
##           18) month: aug,jan,jul,may,nov 9284  5503.00 no ( 0.91265 0.08735 )  
##             36) pdays < 383.5 9246  5373.00 no ( 0.91510 0.08490 )  
##               72) age < 60.5 9097  5107.00 no ( 0.91920 0.08080 )  
##                144) day < 27.5 7670  4588.00 no ( 0.91147 0.08853 )  
##                  288) age < 29.5 754   637.10 no ( 0.85013 0.14987 )  
##                    576) month: jan,jul,may 681   528.00 no ( 0.86931 0.13069 ) *
##                    577) month: aug,nov 73    92.46 no ( 0.67123 0.32877 ) *
##                  289) age > 29.5 6916  3918.00 no ( 0.91816 0.08184 )  
##                    578) balance < 1493 5180  2635.00 no ( 0.92973 0.07027 )  
##                     1156) month: aug,jul,may,nov 5164  2596.00 no ( 0.93087 0.06913 ) *
##                     1157) month: jan 16    21.93 no ( 0.56250 0.43750 ) *
##                    579) balance > 1493 1736  1249.00 no ( 0.88364 0.11636 ) *
##                145) day > 27.5 1427   472.40 no ( 0.96076 0.03924 )  
##                  290) pdays < 184.5 1308   462.50 no ( 0.95719 0.04281 )  
##                    580) pdays < 80.5 1277   402.60 no ( 0.96319 0.03681 ) *
##                    581) pdays > 80.5 31    37.35 no ( 0.70968 0.29032 ) *
##                  291) pdays > 184.5 119     0.00 no ( 1.00000 0.00000 ) *
##               73) age > 60.5 149   190.10 no ( 0.66443 0.33557 ) *
##             37) pdays > 383.5 38    47.40 yes ( 0.31579 0.68421 ) *
##           19) month: apr,feb,jun 2414  2262.00 no ( 0.82187 0.17813 )  
##             38) housing: no 1123  1321.00 no ( 0.72484 0.27516 )  
##               76) day < 9.5 691   668.20 no ( 0.81187 0.18813 )  
##                152) month: feb 505   364.20 no ( 0.88317 0.11683 ) *
##                153) month: apr,jun 186   247.30 no ( 0.61828 0.38172 ) *
##               77) day > 9.5 432   586.10 no ( 0.58565 0.41435 ) *
##             39) housing: yes 1291   803.20 no ( 0.90627 0.09373 )  
##               78) day < 20.5 1210   655.90 no ( 0.92314 0.07686 )  
##                156) month: apr,feb 1154   565.70 no ( 0.93328 0.06672 ) *
##                157) month: jun 56    67.01 no ( 0.71429 0.28571 ) *
##               79) day > 20.5 81   104.40 no ( 0.65432 0.34568 ) *
##        5) month: dec,mar,oct,sep 640   852.70 no ( 0.61562 0.38438 ) *
##      3) poutcome: success 616   806.40 yes ( 0.36201 0.63799 )  
##        6) pdays < 94.5 170   185.50 yes ( 0.23529 0.76471 ) *
##        7) pdays > 94.5 446   603.90 yes ( 0.41031 0.58969 )  
##         14) job: admin.,blue-collar,entrepreneur,services,technician 213   295.20 no ( 0.50704 0.49296 ) *
##         15) job: housemaid,management,retired,self-employed,student,unemployed,unknown 233   292.80 yes ( 0.32189 0.67811 ) *
\end{verbatim}

Based on the structure of the tree, the first variable that has been
used to split the data, is \texttt{poutcome} which is the outcome of the
previous marketing campaign. Also it is worth to note that the variable
\texttt{month} (last contact month of year) has been used frequently for
splitting the data.

For evaluating the performance of the optimal tree we can check the
confusion matrix, accuracy and F1-score for the testing data. F1-score
is better metric to measure the performance of the model as the target
variable \texttt{y} is unbalanced.

\begin{longtable}[]{@{}lrr@{}}
\toprule()
& no & yes \\
\midrule()
\endhead
no & 11872 & 107 \\
yes & 1371 & 214 \\
\bottomrule()
\end{longtable}

The accuracy is 0.891 which indicates that model is very accurate in
predicting new values. However, as mentioned before, since the target
variable has mostly \texttt{no} values, a model which predicts all the
values as \texttt{no} will yield a high accuracy as well. A good model
is a model which can predict \texttt{yes} more accurately in this
dataset. F1-score for this model is 0.224554 which is very low and it
indcates that model is not preforming good in predicting true positives.

One way to increase the performance of the model is to use a loss matrix
to predict the values based on their probabilities (instead of using 0.5
as threshold, we are using \texttt{1/5}). In other words, we are
penalizing the model if it predicts positives wrongly. The confusion
matrix for the testing data and using the loss matrix is as follows:

\begin{longtable}[]{@{}lrr@{}}
\toprule()
& 0 & 1 \\
\midrule()
\endhead
no & 11030 & 949 \\
yes & 771 & 814 \\
\bottomrule()
\end{longtable}

The new F1-score is 0.4862605 which means now the model predict true
positives more accurately although the accuracy has decreased to 0.8732.

One way to compare two different models is to use a ROC curve. Here we
want to compare the optimal decision tree with a logistic regression.

\includegraphics{Lab2-Group_files/figure-latex/unnamed-chunk-16-1.pdf}
Based on the ROC curve, both models are almost preform in the same way,
but decision tree is slightly better. For the same value or \(\pi\) and
same value of False Positive Rate (FPR), the decision tree has higher
value of True Positive Rate.

The precision-recall curve is a better choice to compare two models as
the classes of target data is imbalanced. The model which has the higher
recall for the same value of precision, is more accurate to predict
positive values.

\newpage

\hypertarget{appendix}{%
\subsection{Appendix}\label{appendix}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# ===== Assignment 1=====}

\CommentTok{\# ==Q1==}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}
\FunctionTok{library}\NormalTok{(glmnet)}
\CommentTok{\# Split and Prepare Data}
\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# Split the Data Into Training and Testing Data}
\CommentTok{\# (50/50):}
\NormalTok{tecator }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"tecator.csv"}\NormalTok{)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{12345}\NormalTok{)}
\NormalTok{n }\OtherTok{=} \FunctionTok{nrow}\NormalTok{(tecator)}
\NormalTok{id }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, }\FunctionTok{floor}\NormalTok{(n }\SpecialCharTok{*} \FloatTok{0.5}\NormalTok{))}
\NormalTok{train }\OtherTok{=}\NormalTok{ tecator[id, ]}
\NormalTok{test }\OtherTok{=}\NormalTok{ tecator[}\SpecialCharTok{{-}}\NormalTok{id, ]}

\CommentTok{\# =====Fit Linear Regression=====}
\NormalTok{lm\_tecator }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Fat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . }\SpecialCharTok{{-}}\NormalTok{ Protein }\SpecialCharTok{{-}}\NormalTok{ Moisture }\SpecialCharTok{{-}}
\NormalTok{    Sample, }\AttributeTok{data =}\NormalTok{ train)}

\NormalTok{train\_fitvalues }\OtherTok{\textless{}{-}}\NormalTok{ lm\_tecator}\SpecialCharTok{$}\NormalTok{fitted.values}
\NormalTok{test\_predict }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(lm\_tecator, test)}


\NormalTok{MSE\_train }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(lm\_tecator}\SpecialCharTok{$}\NormalTok{residuals}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{MSE\_test }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{((test}\SpecialCharTok{$}\NormalTok{Fat }\SpecialCharTok{{-}}\NormalTok{ test\_predict)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}


\CommentTok{\# ==Q2,Q3== =====Lasso===== Make argument in the}
\CommentTok{\# format that glmnet can use (data matrix for x)}
\NormalTok{x\_name }\OtherTok{\textless{}{-}} \FunctionTok{colnames}\NormalTok{(tecator)[}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\NormalTok{x\_name }\OtherTok{\textless{}{-}}\NormalTok{ x\_name[}\SpecialCharTok{{-}}\DecValTok{102}\NormalTok{]}
\NormalTok{x\_name }\OtherTok{\textless{}{-}}\NormalTok{ x\_name[}\SpecialCharTok{{-}}\DecValTok{102}\NormalTok{]}
\NormalTok{x\_name }\OtherTok{\textless{}{-}}\NormalTok{ x\_name[}\SpecialCharTok{{-}}\DecValTok{101}\NormalTok{]}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{data.matrix}\NormalTok{(train[, x\_name])}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ train}\SpecialCharTok{$}\NormalTok{Fat}

\CommentTok{\# Train Lasso}
\NormalTok{lasso\_tecator }\OtherTok{\textless{}{-}} \FunctionTok{glmnet}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{alpha =} \DecValTok{1}\NormalTok{)}

\FunctionTok{plot}\NormalTok{(lasso\_tecator, }\AttributeTok{xvar =} \StringTok{"lambda"}\NormalTok{, }\AttributeTok{label =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{main =} \StringTok{"Figure 1:}\SpecialCharTok{\textbackslash{}n}\StringTok{ Dependence of Lasso Regression coefficients on log(lambda) }\SpecialCharTok{\textbackslash{}n\textbackslash{}n}\StringTok{"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{abline}\NormalTok{(}\AttributeTok{v =} \SpecialCharTok{{-}}\FloatTok{0.05}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{abline}\NormalTok{(}\AttributeTok{v =} \SpecialCharTok{{-}}\FloatTok{0.35}\NormalTok{)}

\CommentTok{\# pick lambda for 3 features}
\NormalTok{coef\_matrix }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{coef}\NormalTok{(lasso\_tecator))}
\NormalTok{coef\_matrix }\OtherTok{\textless{}{-}}\NormalTok{ coef\_matrix }\SpecialCharTok{!=} \DecValTok{0}
\NormalTok{coef\_matrix }\OtherTok{\textless{}{-}} \FunctionTok{colSums}\NormalTok{(coef\_matrix)}
\NormalTok{lamda\_index }\OtherTok{\textless{}{-}} \FunctionTok{which}\NormalTok{(coef\_matrix }\SpecialCharTok{==} \DecValTok{4}\NormalTok{)  }\CommentTok{\# we use 4 because there is alway intercept with in the matric}
\NormalTok{lambda\_3features }\OtherTok{\textless{}{-}}\NormalTok{ lasso\_tecator}\SpecialCharTok{$}\NormalTok{lambda[lamda\_index]  }\CommentTok{\# the lamda for 3 features}


\CommentTok{\# ==Q4== =====Ridge======}
\NormalTok{ridge\_tecator }\OtherTok{\textless{}{-}} \FunctionTok{glmnet}\NormalTok{(x, y, }\AttributeTok{alpha =} \DecValTok{0}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(ridge\_tecator, }\AttributeTok{xvar =} \StringTok{"lambda"}\NormalTok{, }\AttributeTok{label =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{main =} \StringTok{"Figure 2:}\SpecialCharTok{\textbackslash{}n}\StringTok{ Dependence of Ridge Regression coefficients on log(lambda) }\SpecialCharTok{\textbackslash{}n\textbackslash{}n}\StringTok{"}\NormalTok{)}

\CommentTok{\# ==Q5==}
\NormalTok{cv\_lasso\_tecator }\OtherTok{\textless{}{-}} \FunctionTok{cv.glmnet}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{alpha =} \DecValTok{1}\NormalTok{)  }\CommentTok{\#Use cv.glmnet to do cross validation}

\FunctionTok{plot}\NormalTok{(cv\_lasso\_tecator, }\AttributeTok{main =} \StringTok{"Figure 3:}\SpecialCharTok{\textbackslash{}n}\StringTok{ Dependence of the Lasso CV score on log lambda}\SpecialCharTok{\textbackslash{}n\textbackslash{}n}\StringTok{"}\NormalTok{)}

\NormalTok{best\_lambda }\OtherTok{\textless{}{-}}\NormalTok{ cv\_lasso\_tecator}\SpecialCharTok{$}\NormalTok{lambda.min}
\NormalTok{best\_model }\OtherTok{\textless{}{-}} \FunctionTok{glmnet}\NormalTok{(x, y, }\AttributeTok{alpha =} \DecValTok{1}\NormalTok{, }\AttributeTok{lambda =}\NormalTok{ best\_lambda)}
\NormalTok{variables }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(best\_model)}
\NormalTok{variables }\OtherTok{\textless{}{-}} \FunctionTok{as.vector}\NormalTok{(variables }\SpecialCharTok{!=} \DecValTok{0}\NormalTok{)}
\NormalTok{coef\_index }\OtherTok{\textless{}{-}} \FunctionTok{which}\NormalTok{(}\FunctionTok{match}\NormalTok{(variables, }\ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)}
\NormalTok{coef\_index }\OtherTok{\textless{}{-}}\NormalTok{ coef\_index[}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}

\NormalTok{coef\_get\_select }\OtherTok{\textless{}{-}} \FunctionTok{rownames}\NormalTok{(}\FunctionTok{coef}\NormalTok{(best\_model))[coef\_index]}

\CommentTok{\# ===== }\RegionMarkerTok{END}\CommentTok{ Assignment 1=====}

\CommentTok{\# ===== Assignment 2 ===== Attaching Packages}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(tree)}

\NormalTok{bank\_full }\OtherTok{\textless{}{-}} \FunctionTok{read.csv2}\NormalTok{(}\StringTok{"./data/bank{-}full.csv"}\NormalTok{)}
\NormalTok{bank\_full }\OtherTok{\textless{}{-}}\NormalTok{ bank\_full }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{select}\NormalTok{(}\SpecialCharTok{!}\NormalTok{duration) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.character), as.factor))}

\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(bank\_full)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{12345}\NormalTok{)}
\NormalTok{train\_idx }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\FunctionTok{seq\_len}\NormalTok{(n), }\FunctionTok{floor}\NormalTok{(n }\SpecialCharTok{*} \FloatTok{0.4}\NormalTok{))}
\NormalTok{train\_data }\OtherTok{\textless{}{-}}\NormalTok{ bank\_full[train\_idx, ]}
\NormalTok{remainder\_idx }\OtherTok{\textless{}{-}} \FunctionTok{setdiff}\NormalTok{(}\FunctionTok{seq\_len}\NormalTok{(n), train\_idx)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{12345}\NormalTok{)}
\NormalTok{valid\_idx }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(remainder\_idx, }\FunctionTok{floor}\NormalTok{(n }\SpecialCharTok{*} \FloatTok{0.3}\NormalTok{))}
\NormalTok{valid\_data }\OtherTok{\textless{}{-}}\NormalTok{ bank\_full[valid\_idx, ]}
\NormalTok{test\_idx }\OtherTok{\textless{}{-}} \FunctionTok{setdiff}\NormalTok{(remainder\_idx, valid\_idx)}
\NormalTok{test\_data }\OtherTok{\textless{}{-}}\NormalTok{ bank\_full[test\_idx, ]}

\CommentTok{\# First Decision Tree:}
\NormalTok{tree\_default }\OtherTok{\textless{}{-}} \FunctionTok{tree}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., train\_data)}
\NormalTok{y\_hat\_train }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(tree\_default, train\_data, }\AttributeTok{type =} \StringTok{"class"}\NormalTok{)}
\NormalTok{cm\_train }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(train\_data}\SpecialCharTok{$}\NormalTok{y, y\_hat\_train)}
\NormalTok{error\_train\_a }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(}\FunctionTok{diag}\NormalTok{(cm\_train))}\SpecialCharTok{/}\FunctionTok{nrow}\NormalTok{(train\_data))}
\CommentTok{\# mean(y\_hat\_train != train\_data$y)}
\NormalTok{y\_hat\_train }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(tree\_default, valid\_data, }\AttributeTok{type =} \StringTok{"class"}\NormalTok{)}
\NormalTok{cm\_valid }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(valid\_data}\SpecialCharTok{$}\NormalTok{y, y\_hat\_train)}
\NormalTok{error\_test\_a }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(}\FunctionTok{diag}\NormalTok{(cm\_valid))}\SpecialCharTok{/}\FunctionTok{nrow}\NormalTok{(valid\_data))}

\CommentTok{\# Second Decision Tree:}
\NormalTok{tree\_minsize }\OtherTok{\textless{}{-}} \FunctionTok{tree}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., train\_data, }\AttributeTok{control =} \FunctionTok{tree.control}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(train\_data),}
    \AttributeTok{minsize =} \DecValTok{7000}\NormalTok{))}
\NormalTok{y\_hat\_train }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(tree\_minsize, train\_data, }\AttributeTok{type =} \StringTok{"class"}\NormalTok{)}
\NormalTok{cm\_train }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(train\_data}\SpecialCharTok{$}\NormalTok{y, y\_hat\_train)}
\NormalTok{error\_train\_b }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(}\FunctionTok{diag}\NormalTok{(cm\_train))}\SpecialCharTok{/}\FunctionTok{nrow}\NormalTok{(train\_data))}
\CommentTok{\# mean(y\_hat\_train != train\_data$y)}
\NormalTok{y\_hat\_train }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(tree\_minsize, valid\_data, }\AttributeTok{type =} \StringTok{"class"}\NormalTok{)}
\NormalTok{cm\_valid }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(valid\_data}\SpecialCharTok{$}\NormalTok{y, y\_hat\_train)}
\NormalTok{error\_test\_b }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(}\FunctionTok{diag}\NormalTok{(cm\_valid))}\SpecialCharTok{/}\FunctionTok{nrow}\NormalTok{(valid\_data))}

\CommentTok{\# Third Decision Tree:}
\NormalTok{tree\_mindev }\OtherTok{\textless{}{-}} \FunctionTok{tree}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., train\_data, }\AttributeTok{control =} \FunctionTok{tree.control}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(train\_data),}
    \AttributeTok{mindev =} \FloatTok{5e{-}04}\NormalTok{))}
\NormalTok{y\_hat\_train }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(tree\_mindev, train\_data, }\AttributeTok{type =} \StringTok{"class"}\NormalTok{)}
\NormalTok{cm\_train }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(train\_data}\SpecialCharTok{$}\NormalTok{y, y\_hat\_train)}
\NormalTok{error\_train\_c }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(}\FunctionTok{diag}\NormalTok{(cm\_train))}\SpecialCharTok{/}\FunctionTok{nrow}\NormalTok{(train\_data))}
\CommentTok{\# mean(y\_hat\_train != train\_data$y)}
\NormalTok{y\_hat\_train }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(tree\_mindev, valid\_data, }\AttributeTok{type =} \StringTok{"class"}\NormalTok{)}
\NormalTok{cm\_valid }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(valid\_data}\SpecialCharTok{$}\NormalTok{y, y\_hat\_train)}
\NormalTok{error\_test\_c }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(}\FunctionTok{diag}\NormalTok{(cm\_valid))}\SpecialCharTok{/}\FunctionTok{nrow}\NormalTok{(valid\_data))}


\NormalTok{train\_dev }\OtherTok{\textless{}{-}} \FunctionTok{vector}\NormalTok{(}\StringTok{"numeric"}\NormalTok{, }\DecValTok{50}\NormalTok{)}
\NormalTok{valid\_dev }\OtherTok{\textless{}{-}} \FunctionTok{vector}\NormalTok{(}\StringTok{"numeric"}\NormalTok{, }\DecValTok{50}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ (num\_leaves }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\DecValTok{50}\NormalTok{) \{}
\NormalTok{    prune\_tree }\OtherTok{\textless{}{-}} \FunctionTok{prune.tree}\NormalTok{(tree\_mindev, }\AttributeTok{best =}\NormalTok{ num\_leaves)}
\NormalTok{    valid\_tree }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(prune\_tree, }\AttributeTok{newdata =}\NormalTok{ valid\_data,}
        \AttributeTok{type =} \StringTok{"tree"}\NormalTok{)}
\NormalTok{    train\_dev[num\_leaves] }\OtherTok{\textless{}{-}} \FunctionTok{deviance}\NormalTok{(prune\_tree)}
\NormalTok{    valid\_dev[num\_leaves] }\OtherTok{\textless{}{-}} \FunctionTok{deviance}\NormalTok{(valid\_tree)}
\NormalTok{\}}

\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{num\_of\_leaves =} \DecValTok{2}\SpecialCharTok{:}\DecValTok{50}\NormalTok{, }\AttributeTok{train =}\NormalTok{ train\_dev[}\DecValTok{2}\SpecialCharTok{:}\DecValTok{50}\NormalTok{],}
    \AttributeTok{valid =}\NormalTok{ valid\_dev[}\DecValTok{2}\SpecialCharTok{:}\DecValTok{50}\NormalTok{]) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(num\_of\_leaves, train, }\AttributeTok{color =} \StringTok{"Train Deviance"}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(num\_of\_leaves, train, }\AttributeTok{color =} \StringTok{"Train Deviance"}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(num\_of\_leaves, valid, }\AttributeTok{color =} \StringTok{"Validation Deviance"}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(num\_of\_leaves, valid, }\AttributeTok{color =} \StringTok{"Validation Deviance"}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Number of Leaves"}\NormalTok{, }\AttributeTok{y =} \StringTok{"deviance"}\NormalTok{, }\AttributeTok{color =} \StringTok{"dataset"}\NormalTok{)}
\NormalTok{best\_num\_leaves }\OtherTok{\textless{}{-}} \FunctionTok{which.min}\NormalTok{(valid\_dev[}\DecValTok{2}\SpecialCharTok{:}\DecValTok{50}\NormalTok{]) }\SpecialCharTok{+} \DecValTok{1}
\NormalTok{optimal\_tree }\OtherTok{\textless{}{-}} \FunctionTok{prune.tree}\NormalTok{(tree\_mindev, }\AttributeTok{best =}\NormalTok{ best\_num\_leaves)}

\CommentTok{\# structure of tree}
\NormalTok{optimal\_tree}

\NormalTok{y\_hat }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(optimal\_tree, }\AttributeTok{newdata =}\NormalTok{ test\_data, }\AttributeTok{type =} \StringTok{"class"}\NormalTok{)}

\NormalTok{cm\_test }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(test\_data}\SpecialCharTok{$}\NormalTok{y, y\_hat)}
\NormalTok{acc }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{diag}\NormalTok{(cm\_test))}\SpecialCharTok{/}\FunctionTok{nrow}\NormalTok{(test\_data)}

\NormalTok{TP }\OtherTok{\textless{}{-}}\NormalTok{ cm\_test[}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{]}
\NormalTok{FP }\OtherTok{\textless{}{-}}\NormalTok{ cm\_test[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{]}
\NormalTok{FN }\OtherTok{\textless{}{-}}\NormalTok{ cm\_test[}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{]}

\NormalTok{F1\_score }\OtherTok{\textless{}{-}}\NormalTok{ TP}\SpecialCharTok{/}\NormalTok{(TP }\SpecialCharTok{+} \FloatTok{0.5} \SpecialCharTok{*}\NormalTok{ (FP }\SpecialCharTok{+}\NormalTok{ FN))}

\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(cm\_test, }\AttributeTok{label =} \StringTok{"the confusion matrix of optimal tree on test data"}\NormalTok{)}


\NormalTok{y\_hat\_prob\_tree }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(optimal\_tree, test\_data)}
\NormalTok{y\_hat\_prob\_tree }\OtherTok{\textless{}{-}}\NormalTok{ y\_hat\_prob\_tree[, }\StringTok{"yes"}\NormalTok{]}

\NormalTok{glm\_model }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{family =} \StringTok{"binomial"}\NormalTok{, }\AttributeTok{data =}\NormalTok{ train\_data)}
\NormalTok{y\_hat\_prob\_glm }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(glm\_model, test\_data, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}

\NormalTok{pis }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\FloatTok{0.95}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.05}\NormalTok{)}

\NormalTok{ROC\_tree }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\AttributeTok{nrow =} \FunctionTok{length}\NormalTok{(pis), }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{)}
\NormalTok{ROC\_glm }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\AttributeTok{nrow =} \FunctionTok{length}\NormalTok{(pis), }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \FunctionTok{seq\_along}\NormalTok{(pis)) \{}

\NormalTok{    y\_hat\_tree }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(y\_hat\_prob\_tree }\SpecialCharTok{\textgreater{}}\NormalTok{ pis[i],}
        \StringTok{"yes"}\NormalTok{, }\StringTok{"no"}\NormalTok{), }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"no"}\NormalTok{, }\StringTok{"yes"}\NormalTok{))}
\NormalTok{    cm\_test }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(test\_data}\SpecialCharTok{$}\NormalTok{y, y\_hat\_tree)}
\NormalTok{    TP }\OtherTok{\textless{}{-}}\NormalTok{ cm\_test[}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{]}
\NormalTok{    FP }\OtherTok{\textless{}{-}}\NormalTok{ cm\_test[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{]}
\NormalTok{    ROC\_tree[i, }\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ FP}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(cm\_test[}\DecValTok{1}\NormalTok{, ])}
\NormalTok{    ROC\_tree[i, }\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ TP}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(cm\_test[}\DecValTok{2}\NormalTok{, ])}

\NormalTok{    y\_hat\_glm }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(y\_hat\_prob\_glm }\SpecialCharTok{\textgreater{}}\NormalTok{ pis[i],}
        \StringTok{"yes"}\NormalTok{, }\StringTok{"no"}\NormalTok{), }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"no"}\NormalTok{, }\StringTok{"yes"}\NormalTok{))}
\NormalTok{    cm\_test }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(test\_data}\SpecialCharTok{$}\NormalTok{y, y\_hat\_glm)}
\NormalTok{    TP }\OtherTok{\textless{}{-}}\NormalTok{ cm\_test[}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{]}
\NormalTok{    FP }\OtherTok{\textless{}{-}}\NormalTok{ cm\_test[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{]}
\NormalTok{    ROC\_glm[i, }\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ FP}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(cm\_test[}\DecValTok{1}\NormalTok{, ])}
\NormalTok{    ROC\_glm[i, }\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ TP}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(cm\_test[}\DecValTok{2}\NormalTok{, ])}

\NormalTok{\}}

\FunctionTok{plot}\NormalTok{(ROC\_tree, }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.8}\NormalTok{),}
    \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{xlab =} \StringTok{"FPR"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"TPR"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(ROC\_glm, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"GLM"}\NormalTok{, }\StringTok{"Decision Tree"}\NormalTok{), }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{,}
    \StringTok{"blue"}\NormalTok{), }\AttributeTok{lty =} \DecValTok{1}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}


\end{document}
